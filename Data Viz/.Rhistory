mean(76, -56,28,101,8, -24,47,98,15, -39)
mode(76, -56,28,101,8, -24,47,98,15, -39)
median(76, -56,28,101,8, -24,47,98,15, -39)
x <- c(76, -56,28,101,8, -24,47,98,15, -39)
y <- c(3,12,15,8.9,23,0.78,18,45,0,86,7,6.9,5,35,20)
z <- c(2/5,6/5,25/5,33/12,89/7,23/4,63/33,6/3)
mean(x)
mean(y)
mean(z)
median(x)
median(y)
median(z)
source('~/.active-rstudio-document')
6/3
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(x)
var(x)
var(y)
var(z)
sd(x)
sd(y)
sd(z)
max(x) - min(x)
max(y) - min(y)
max(z) - min(z)
install.packages("rmarkdown")
library(rmarkdown)
install.packages("caret")
install.packages("mlbench")
install.packages("AppliedPredictiveModeling")
install.packages("mlbench")
# load libraries
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# summarize pedigree and age
summary(PimaIndiansDiabetes[,7:8])
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c("BoxCox"))
install.packages("e1071")
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c("BoxCox"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])
# summarize the transformed dataset (note pedigree and age)
summary(transformed)
install.packages("klaR")
library(caret)
# load the iris dataset
data(iris)
# define training control
#uses a bootstrap with 100 resamples to estimate the accuracy of a Naive Bayes model.
train_control <- trainControl(method="boot", number=100)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
library(caret)
library(klaR)
data(iris)
# define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(iris$Species, p=0.80, list=FALSE)
dataTrain <- iris[ trainIndex,]
dataTest <- iris[-trainIndex,]
# train a naive Bayes model
fit <- NaiveBayes(Species~., data=dataTrain)
# make predictions
predictions <- predict(fit, dataTest[,1:4])
# summarize results
confusionMatrix(predictions$class, dataTest$Species)
library(caret)
data(iris)
# define training control
trainControl <- trainControl(method="boot", number=100)
# evalaute the model
fit <- train(Species~., data=iris, trControl=trainControl, method="nb")
# display the results
print(fit)
library(caret)
data(iris)
# define training control
trainControl <- trainControl(method="cv", number=10)
# evaluate the model
fit <- train(Species~., data=iris, trControl=trainControl, method="nb")
# display the results
print(fit)
library(caret)
data(iris)
# define training control
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
# evaluate the model
fit <- train(Species~., data=iris, trControl=trainControl, method="nb")
# display the results
print(fit)
library(caret)
data(iris)
# define training control
trainControl <- trainControl(method="LOOCV")
# evaluate the model
fit <- train(Species~., data=iris, trControl=trainControl, method="nb")
# display the results
print(fit)
setwd("C:/Users/Traci/Google Drive/Programming/R/Chapter 06")
## Understanding regression ----
## Example: Space Shuttle Launch Data ----
launch <- read.csv("challenger.csv")
# estimate beta manually
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
b
# confirming the regression line using the lm function (not in text)
model <- lm(distress_ct ~ temperature, data = launch)
model
summary(model)
# creating a simple multiple regression function
reg <- function(y, x) {
x <- as.matrix(x)
x <- cbind(Intercept = 1, x)
b <- solve(t(x) %*% x) %*% t(x) %*% y
colnames(b) <- "estimate"
print(b)
}
reg
# examine the launch data
str(launch)
# test regression model with simple linear regression
reg(y = launch$distress_ct, x = launch[2])
# use regression model with multiple regression
reg(y = launch$distress_ct, x = launch[2:4])
# confirming the multiple regression result using the lm function (not in text)
model <- lm(distress_ct ~ temperature + pressure + launch_id, data = launch)
model
## Example: Predicting Medical Expenses ----
## Step 2: Exploring and preparing the data ----
insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE)
str(insurance)
# histogram of insurance charges
hist(insurance$expenses)
# it can be useful to determine how the independent variables are related to the dependent variable and each other.
# exploring relationships among features: correlation matrix
cor(insurance[c("age", "bmi", "children", "expenses")])
# visualing relationships among features: scatterplot matrix
pairs(insurance[c("age", "bmi", "children", "expenses")])
install.packages("psych")
# more informative scatterplot matrix
library(psych)
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])
## Step 3: Training a model on the data ----
ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region,
data = insurance)
ins_model <- lm(expenses ~ ., data = insurance) # this is equivalent to above
ins_model
## Step 4: Evaluating model performance ----
# see more detail about the estimated beta coefficients
summary(ins_model)
## Step 5: Improving model performance ----
# -> the relationship between an independent variable and the dependent variable is assumed to be linear,
#    yet this may not necessarily be true.
# -> To account for a non-linear relationship,
# add a higher-order "age" term
insurance$age2 <- insurance$age^2
# Transformation â€“ converting a numeric variable to a binary indicator
# -> create a binary obesity indicator variable that is 1 if the BMI is at least 30, and 0 if less.
# add an indicator for BMI >= 30
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
# create final model
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex +
bmi30*smoker + region, data = insurance)
summary(ins_model2)
library(mlbench)
data(BostonHousing)
# fit model
fit <- lm(medv~., BostonHousing)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, BostonHousing)
predictions
# summarize accuracy
mse <- mean((BostonHousing$medv - predictions)^2)
print(mse)
library(caret)
library(mlbench)
# load dataset
data(BostonHousing)
# train
set.seed(7)
trainControl <- trainControl(method="cv", number=5)
fit.lm <- train(medv~., data=BostonHousing, method="lm", metric="RMSE", preProc=c("center",
"scale"), trControl=trainControl)
# summarize fit
print(fit.lm)
# Logistic Regression
# creates a generalized linear model for regression or classification.
library(mlbench)
data(PimaIndiansDiabetes)
# fit model
fit <- glm(diabetes~., data=PimaIndiansDiabetes, family=binomial(link='logit'))
# summarize the fit
print(fit)
# make predictions
probabilities <- predict(fit, PimaIndiansDiabetes[,1:8], type='response')
predictions <- ifelse(probabilities > 0.5,'pos','neg')
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)
library(caret)
library(mlbench)
data(PimaIndiansDiabetes)
# train
set.seed(7)
trainControl <- trainControl(method="cv", number=5)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="lm", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glm)
library(caret)
library(mlbench)
data(PimaIndiansDiabetes)
# train
set.seed(7)
trainControl <- trainControl(method="cv", number=5)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="lm", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glm)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="polr", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glm)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="bayesglm", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="bayesglm", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glm)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="LogitBoost", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glm)
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="gpls", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
n
a
fit.glm <- train(diabetes~., data=PimaIndiansDiabetes, method="gpls", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glm)
# Linear Discriminant Analysis
library(MASS)
library(mlbench)
data(PimaIndiansDiabetes)
fit <- lda(diabetes~., data=PimaIndiansDiabetes)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, PimaIndiansDiabetes[,1:8])$class
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)
library(caret)
library(mlbench)
data(PimaIndiansDiabetes)
# train
set.seed(7)
trainControl <- trainControl(method="cv", number=5)
fit.lda <- train(diabetes~., data=PimaIndiansDiabetes, method="lda", metric="Accuracy",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.lda)
# Regularized Regression
library(glmnet)
install.packages("glmnet")
# Regularized Regression
library(glmnet)
library(mlbench)
data(BostonHousing)
BostonHousing$chas <- as.numeric(as.character(BostonHousing$chas))
x <- as.matrix(BostonHousing[,1:13])
y <- as.matrix(BostonHousing[,14])
# fit model
fit <- glmnet(x, y, family="gaussian", alpha=0.5, lambda=0.001)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, x, type="link")
# summarize accuracy
mse <- mean((y - predictions)^2)
print(mse)
# can also be configured to perform three important types of regularization: lasso, ridge and elastic net by configuring the alpha parameter to 1, 0 or in [0,1] respectively.
library(caret)
library(mlbench)
library(glmnet)
data(PimaIndiansDiabetes)
# train
set.seed(7)
trainControl <- trainControl(method="cv", number=5)
fit.glmnet <- train(diabetes~., data=PimaIndiansDiabetes, method="glmnet",
metric="Accuracy", preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glmnet)
## Understanding regression trees and model trees ----
## Example: Calculating SDR ----
# set up the data
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
library(caret)
library(mlbench)
library(glmnet)
# Load the dataset
data(BostonHousing)
# train
set.seed(7)
trainControl <- trainControl(method="cv", number=5)
fit.glmnet <- train(medv~., data=BostonHousing, method="glmnet", metric="RMSE",
preProc=c("center", "scale"), trControl=trainControl)
# summarize fit
print(fit.glmnet)
# Example of GLMNET algorithm for classification
library(glmnet)
library(mlbench)
data(PimaIndiansDiabetes)
x <- as.matrix(PimaIndiansDiabetes[,1:8])
y <- as.matrix(PimaIndiansDiabetes[,9])
# fit model
fit <- glmnet(x, y, family="binomial", alpha=0.5, lambda=0.001)
# summarize the fit
print(fit)
# make predictions
predictions <- predict(fit, x, type="class")
# summarize accuracy
table(predictions, PimaIndiansDiabetes$diabetes)
